{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Importing Required Libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from uszipcode import Zipcode, SearchEngine\n",
    "search = SearchEngine(simple_zipcode=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets, linear_model, metrics \n",
    "import statsmodels.api as sm\n",
    "# from scipy import stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.datasets import make_regression\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "%matplotlib inline\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\Amit Gupta\\\\Desktop\\\\Airbnb\")\n",
    "\n",
    "# pd.options.display.max_columns = 5\n",
    "# pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Importing data and Data cleaning (i.e. handling missing data, etc)\n",
    "\n",
    "### 2.a) Importing Data\n",
    "\n",
    "df_train = pd.read_csv(\"./Data/train10k.csv\")\n",
    "df_train.head()\n",
    "\n",
    "df_test = pd.read_csv(\"./Data/test2k.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.b) Check for missing values\n",
    "\n",
    "# Checked for scaling of the variables. There was almost no change in the R2 values of the model after scaling the variables. Therefore, not performing scaling.\n",
    "# Checking for missing values.\n",
    "df_missing = df_train.isnull().sum(axis=0).reset_index() # Finding for the count of null values for each of the variable in the dataset.\n",
    "df_missing.columns = ['column_name', 'missing_count'] # Renaming the column.\n",
    "df_missing = df_missing.loc[df_missing['missing_count']>0] # Considering or displaying only those columns which have null values.\n",
    "df_missing = df_missing.sort_values(by='missing_count') # Sorting the variables w.r.t. to the count of null values in the variables (in asc order)\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.c) Handling missing values in both train and test dataframe\n",
    "\n",
    "# Creating a function to replace missing values with zero.\n",
    "def handleMissingNullValues(dataframes,cols):\n",
    "    for df in dataframes:\n",
    "        for col in cols:\n",
    "            df[col]=df[col].fillna(0)\n",
    "\n",
    "# Using handleMissingNullValues() function to handle missing values for [\"bathrooms\",\"bedrooms\",\"beds\",\"review_scores_rating\"] variables\n",
    "handleMissingNullValues([df_train,df_test],[\"bathrooms\",\"bedrooms\",\"beds\",\"review_scores_rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['host_has_profile_pic'] = df_train['host_has_profile_pic'].fillna('f')\n",
    "df_train['host_identity_verified'] = df_train['host_identity_verified'].fillna('f')\n",
    "df_train['instant_bookable'] = df_train['instant_bookable'].fillna('f')\n",
    "df_test['host_has_profile_pic'] = df_test['host_has_profile_pic'].fillna('f')\n",
    "df_test['host_identity_verified'] = df_test['host_identity_verified'].fillna('f')\n",
    "df_test['instant_bookable'] = df_test['instant_bookable'].fillna('f')\n",
    "\n",
    "# Using key value pair to replace values for some variables and then to map it back to the original variable by using inplace=TRUE\n",
    "cleanup_nums = {\"host_has_profile_pic\": \n",
    "                    {\n",
    "                        \"t\": True,\n",
    "                        \"f\": False\n",
    "                    },\n",
    "                \"host_identity_verified\": \n",
    "                    {\n",
    "                        \"t\": True,\n",
    "                        \"f\": False\n",
    "                    },\n",
    "                \"instant_bookable\":\n",
    "                    {\n",
    "                        \"t\": True,\n",
    "                        \"f\": False                    \n",
    "                    }\n",
    "                }\n",
    "df_train.replace(cleanup_nums, inplace=True)\n",
    "df_test.replace(cleanup_nums, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToBoolean(dataframes,cols):\n",
    "    for df in dataframes:\n",
    "        for col in cols:\n",
    "            df[col] = df[col].astype(bool)\n",
    "\n",
    "convertToBoolean([df_train,df_test],[\"host_has_profile_pic\",\"host_identity_verified\",\"instant_bookable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'neighbourhood'\n",
    "# Creating new class or group of neighbourhood. Assigning it the value as Unknown\n",
    "df_train['neighbourhood'] = df_train['neighbourhood'].fillna('Unknown')\n",
    "df_test['neighbourhood'] = df_test['neighbourhood'].fillna('Unknown')\n",
    "\n",
    "# 'host_response_rate'\n",
    "# Removing the % sign from the variable so that we can fill the null values. Replacing % with nothing.\n",
    "df_train['host_response_rate'] = df_train['host_response_rate'].str.replace('%','')\n",
    "# Filling the null values with 0.\n",
    "df_train['host_response_rate'] = df_train['host_response_rate'].fillna('0')\n",
    "# Converting the variable values into numeric using the pd.to_numeric() function.\n",
    "df_train['host_response_rate'] = pd.to_numeric(df_train['host_response_rate'])\n",
    "df_test['host_response_rate'] = df_test['host_response_rate'].str.replace('%','')\n",
    "df_test['host_response_rate'] = df_test['host_response_rate'].fillna('0')\n",
    "df_test['host_response_rate'] = pd.to_numeric(df_test['host_response_rate'])\n",
    "\n",
    "# 'thumbnail_url'\n",
    "df_train['thumbnail_url'] = df_train['thumbnail_url'].notnull()\n",
    "df_train['thumbnail_url'] = df_train['thumbnail_url'].astype(bool)\n",
    "df_test['thumbnail_url'] = df_test['thumbnail_url'].notnull()\n",
    "df_test['thumbnail_url'] = df_test['thumbnail_url'].astype(bool)\n",
    "\n",
    "df_train['zipcode'] = df_train['zipcode'].fillna(0)\n",
    "df_train.loc[df_train.zipcode == ' ', 'zipcode'] = 0\n",
    "blank_zip = df_train.index[df_train['zipcode']==0].tolist()\n",
    "for i in blank_zip:\n",
    "    lat = df_train['latitude'][i]\n",
    "    lon = df_train['longitude'][i]\n",
    "    result = np.max(search.by_coordinates(lat, lon, radius=30, returns=5))\n",
    "    df_train['zipcode'][i]=result.values()[0]    \n",
    "\n",
    "df_test['zipcode'] = df_test['zipcode'].fillna(0)\n",
    "df_test.loc[df_test.zipcode == ' ', 'zipcode'] = 0\n",
    "blank_zip = df_test.index[df_test['zipcode']==0].tolist()\n",
    "for i in blank_zip:\n",
    "    lat = df_test['latitude'][i]\n",
    "    lon = df_test['longitude'][i]\n",
    "    result = np.max(search.by_coordinates(lat, lon, radius=30, returns=5))\n",
    "    df_test['zipcode'][i]=result.values()[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again check for missing values in the train dataframe\n",
    "df_missing = df_train.isnull().sum(axis=0).reset_index()\n",
    "df_missing.columns = ['column_name', 'missing_count']\n",
    "df_missing = df_missing.loc[df_missing['missing_count']>0]\n",
    "df_missing = df_missing.sort_values(by='missing_count')\n",
    "df_missing\n",
    "\n",
    "# Again check for missing values in the test dataframe\n",
    "df_missing_test = df_test.isnull().sum(axis=0).reset_index()\n",
    "df_missing_test.columns = ['column_name', 'missing_count']\n",
    "df_missing_test = df_missing_test.loc[df_missing_test['missing_count']>0]\n",
    "df_missing_test = df_missing_test.sort_values(by='missing_count')\n",
    "df_missing_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting log prices into normal prices.\n",
    "df_train['int_price'] = np.exp(df_train['log_price'])\n",
    "df_train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Data Visualization\n",
    "### 3.a) Plot 1: Integer Pricing per City\n",
    "\n",
    "plot1 = sns.barplot(x='city',y='int_price',data=df_train)\n",
    "fig = plot1.get_figure()\n",
    "fig.savefig('./Plots/Int_Price_Per_City.png')\n",
    "\n",
    "### 3.b) Plot 2: Average pricing for Property Type w.r.to City\n",
    "\n",
    "df_train.pivot_table(values='int_price',index='property_type',columns='city') # We will get average prices for each property time for all the cities.\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "pivot_df = df_train.pivot_table(values='int_price',index='property_type',columns='city')\n",
    "plot2 = sns.heatmap(pivot_df, annot=True, cmap=\"Reds\",fmt=\".02f\")\n",
    "fig = plot2.get_figure()\n",
    "fig.savefig('./Plots/AvgPrice_PropType_City.png')\n",
    "\n",
    "### 3.c) Plot 3: Price variation for Property Type as per Accomodates\n",
    "df_train.pivot_table(values='int_price',index='property_type',columns='accommodates') \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "pivot_df = df_train.pivot_table(values='int_price',index='property_type',columns='accommodates')\n",
    "plot3 = sns.heatmap(pivot_df, annot=True, cmap=\"Reds\",fmt=\".02f\")\n",
    "fig = plot3.get_figure()\n",
    "fig.savefig('./Plots/PriceVariation_PropertyType_Accom.png')\n",
    "\n",
    "### 3.d) Plot 4: Average price w.r.t. number of rooms as per city\n",
    "df_train.pivot_table(values='int_price',index='bedrooms',columns='city') \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "pivot_df = df_train.pivot_table(values='int_price',index='bedrooms',columns='city')\n",
    "plot3 = sns.heatmap(pivot_df, annot=True, cmap=\"Reds\",fmt=\".02f\")\n",
    "fig = plot3.get_figure()\n",
    "fig.savefig('./Plots/NoOfRooms_City_AvgPrice.png')\n",
    "\n",
    "### 3.e) Plot 5: No of reviews per city for every room type\n",
    "df_train.pivot_table(values='number_of_reviews',index='city',columns='room_type', aggfunc=np.sum) \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "pivot_df = df_train.pivot_table(values='number_of_reviews',index='bedrooms',columns='city',aggfunc=np.sum)\n",
    "plot3 = sns.heatmap(pivot_df, annot=True, cmap=\"Reds\",fmt=\".02f\")\n",
    "fig = plot3.get_figure()\n",
    "fig.savefig('./Plots/CountReviews_City_RoomType.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================  Feature Engineering  =========================================\n",
    "\n",
    "# finding the most recent date in the dataset:\n",
    "df_train['host_since'] = pd.to_datetime(df_train['host_since'])\n",
    "recent_host_since = df_train['host_since'].max()\n",
    "df_train['first_review'] = pd.to_datetime(df_train['first_review'])\n",
    "recent_first_review = df_train['first_review'].max()\n",
    "df_train['last_review'] = pd.to_datetime(df_train['last_review'])\n",
    "recent_last_review = df_train['last_review'].max()\n",
    "t = []\n",
    "t.append([recent_host_since,recent_first_review,recent_last_review])\n",
    "# Therefore, the most recent date in the dataset is \"09-12-2017\". This date is also the most recent in the testing set.\n",
    "\n",
    "# Converting most recent date into required format.\n",
    "import re\n",
    "def change_date_format(dt):\n",
    "        return re.sub(r'(\\d{4})-(\\d{1,2})-(\\d{1,2})', '\\\\3-\\\\2-\\\\1', dt)\n",
    "dt1 = \"2017-12-09\" \n",
    "print(\"Original date in YYY-MM-DD Format: \",dt1)\n",
    "print(\"New date in DD-MM-YYYY Format: \",change_date_format(dt1))\n",
    "dt1 = change_date_format(dt1) # Getting today's date in dd-mm-yyyy format.\n",
    "dt1 = pd.to_datetime(dt1)\n",
    "\n",
    "def convertStringToDays(dataframes,cols,newcols):\n",
    "    for df in dataframes:\n",
    "        for index,col in enumerate(cols):\n",
    "            df[col] =  pd.to_datetime(df[col]) \n",
    "            newcolumn= newcols[index]\n",
    "            df[newcolumn] = dt1 - df[col]\n",
    "            df[newcolumn] = df[newcolumn].astype('timedelta64[D]')  # Fetching only the number of days from datetime format.\n",
    "            df[newcolumn] = df[newcolumn].fillna(0)\n",
    "\n",
    "\n",
    "convertStringToDays([df_train,df_test],[\"host_since\",\"first_review\",\"last_review\"],[\"host_since_days\",\"days_since_first_review\",\"days_since_last_review\"])\n",
    "\n",
    "df_train.info()\n",
    "df_test.info()\n",
    "\n",
    "df_train.to_csv(\"./Tableau/Data_For_Tableau.csv\")\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "corr = df_train.corr()\n",
    "hm = sns.heatmap(round(corr,2),annot=True,cmap=\"Reds\", fmt=\".02f\")\n",
    "fig = hm.get_figure()\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "# We are getting the below variables as correlated with correlation greater than 0.70.\n",
    "# Zipcode and latitude = -0.87\n",
    "# zipcode and longitude = -0.99\n",
    "# Latitude and Longitude = 0.90\n",
    "# Accomodates and bedrooms = 0.71\n",
    "# Accomodates and beds = 0.82\n",
    "# Bedrooms and Beds = 0.71\n",
    "# Log_price and int_price = 0.84\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ Working on Train Dataset  =================================================\n",
    "\n",
    "# Selecting Variables for model\n",
    "temp_train = df_train.copy()\n",
    "temp_train.info()\n",
    "\n",
    "temp_train = temp_train.drop(['description','amenities','name','first_review','host_since','last_review','neighbourhood','int_price'],axis=1)\n",
    "\n",
    "property_type_dummy = pd.get_dummies(temp_train['property_type'],prefix='property_type')\n",
    "room_type_dummy = pd.get_dummies(temp_train['room_type'],prefix='room_type')\n",
    "bed_type_dummy = pd.get_dummies(temp_train['bed_type'],prefix='bed_type')\n",
    "cancellation_policy_dummy = pd.get_dummies(temp_train['cancellation_policy'],prefix='cancellation_policy')\n",
    "city_dummy = pd.get_dummies(temp_train['city'],prefix='city')\n",
    "\n",
    "temp_train = temp_train.drop(['property_type','room_type','bed_type','cancellation_policy','city'],axis=1)\n",
    "temp_train.info()\n",
    "\n",
    "temp_train1 = pd.concat([property_type_dummy,room_type_dummy,bed_type_dummy,cancellation_policy_dummy,city_dummy],axis=1)\n",
    "temp_train1.info()\n",
    "\n",
    "temp_train2 = pd.concat([temp_train1,temp_train],axis=1)\n",
    "temp_train2.info()\n",
    "\n",
    "# Correlation of the Final Merged Dataset with dummies.\n",
    "plt.figure(figsize=(110,55))\n",
    "corr = temp_train2.drop(['log_price'],axis=1).corr()\n",
    "hm = sns.heatmap(round(corr,2),annot=True,cmap=\"Reds\", fmt=\".02f\")\n",
    "fig = hm.get_figure()\n",
    "plt.title(\"Correlation Matrix\")\n",
    "\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=10):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Correlated Value Pairs\")\n",
    "print(get_top_abs_correlations(temp_train2.drop(['log_price'],axis=1),22)) # Getting correlated variable pairs with correlation > 0.50.\n",
    "\n",
    "top_corr = get_top_abs_correlations(temp_train2.drop(['log_price'],axis=1), 22)\n",
    "top_corr.to_csv('./Data/top_corr.csv')\n",
    "# =============================================================================\n",
    "# Correlated pairs\n",
    "# =============================================================================\n",
    "# longitude                     zipcode                       0.988571\n",
    "# room_type_Entire home/apt     room_type_Private room        0.943475\n",
    "# city_LA                       latitude                      0.939557\n",
    "# latitude                      longitude                     0.895887\n",
    "# latitude                      zipcode                       0.869091\n",
    "# accommodates                  beds                          0.820039\n",
    "# city_LA                       longitude                     0.788718\n",
    "# city_NYC                      zipcode                       0.777360\n",
    "# city_LA                       zipcode                       0.777015\n",
    "# city_NYC                      longitude                     0.745928\n",
    "# property_type_Apartment       property_type_House           0.744267\n",
    "# bedrooms                      beds                          0.713737\n",
    "# accommodates                  bedrooms                      0.710387\n",
    "# city_NYC                      latitude                      0.654099\n",
    "# bed_type_Futon                bed_type_Real Bed             0.634489\n",
    "# bathrooms                     bedrooms                      0.585378\n",
    "# cancellation_policy_flexible  cancellation_policy_strict    0.582007\n",
    "# city_LA                       city_NYC                      0.578890\n",
    "# cancellation_policy_moderate  cancellation_policy_strict    0.521276\n",
    "# number_of_reviews             days_since_first_review       0.512281\n",
    "# bathrooms                     beds                          0.503537\n",
    "# bed_type_Pull-out Sofa        bed_type_Real Bed             0.503072\n",
    "# =============================================================================\n",
    "\n",
    "# Checking the feature importance\n",
    "X = temp_train2.drop(['log_price'],axis=1)\n",
    "y = temp_train2['log_price']\n",
    "\n",
    "# Calculating the feature importance score using Extra Trees Regressor:\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "model = ExtraTreesRegressor()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of Extra Trees Regressor\n",
    "#plot graph of feature importances for better visualization\n",
    "plt.figure(figsize=(50,20))\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(68).plot(kind='barh')\n",
    "plt.show()\n",
    "feat_importances.to_csv(\"./Data/importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ FEATURE REDUCTION  =================================================\n",
    "# Removing correlated variables to avoid multi-collinearity => therefore removed = ['latitude','longitude','city_SF', 'city_DC', 'city_LA', 'city_NYC', 'city_Chicago', 'city_Boston','beds','accommodates','bedrooms','days_since_last_review','days_since_first_review']\n",
    "\n",
    "# Removing multicollinearity in the data by removing the variable having less relevant score out of the correlated variables.\n",
    "# KEEPING ZIPCODE OUT OF ALL OTHER LOCATION PARAMETERS AND ALSO KEEPING THE ROOM_TYPE AND PROPERTY_TYPE DUMMY IN THE MODEL(NOT REMOVING IT)\n",
    "#temp3 = temp2.drop(['latitude','longitude','city_SF', 'city_DC', 'city_LA', 'city_NYC', 'city_Chicago', 'city_Boston','room_type_Private room','beds','property_type_House','accommodates','bed_type_Futon','days_since_last_review','days_since_first_review'],axis=1)\n",
    "temp_train3 = temp_train2.drop(['latitude','longitude','city_SF', 'city_DC', 'city_LA', 'city_NYC', 'city_Chicago', 'city_Boston','beds','accommodates','bedrooms','days_since_last_review','days_since_first_review'],axis=1)\n",
    "\n",
    "temp_train3.info()\n",
    "\n",
    "final = temp_train3.copy()\n",
    "# temp_train3 = final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ Working on Train Dataset Complete  =================================================\n",
    "\n",
    "# ============================ Working on Test Dataset  =================================================\n",
    "temp_test = df_test.copy()\n",
    "temp_test.info()\n",
    "\n",
    "temp_test = temp_test.drop(['description','amenities','name','first_review','host_since','last_review','neighbourhood','host_since','first_review','last_review'],axis=1)\n",
    "\n",
    "property_type_dummy_test = pd.get_dummies(temp_test['property_type'],prefix='property_type')\n",
    "room_type_dummy_test = pd.get_dummies(temp_test['room_type'],prefix='room_type')\n",
    "bed_type_dummy_test = pd.get_dummies(temp_test['bed_type'],prefix='bed_type')\n",
    "cancellation_policy_dummy_test = pd.get_dummies(temp_test['cancellation_policy'],prefix='cancellation_policy')\n",
    "city_dummy_test = pd.get_dummies(temp_test['city'],prefix='city')\n",
    "\n",
    "temp_test = temp_test.drop(['property_type','room_type','bed_type','cancellation_policy','city'],axis=1)\n",
    "temp_test.info()\n",
    "\n",
    "temp_test1 = pd.concat([property_type_dummy_test,room_type_dummy_test,bed_type_dummy_test,cancellation_policy_dummy_test,city_dummy_test],axis=1)\n",
    "temp_test1.info()\n",
    "\n",
    "temp_test2 = pd.concat([temp_test1,temp_test],axis=1)\n",
    "temp_test2.info()\n",
    "\n",
    "temp_test3 = temp_test2.drop(['latitude','longitude','city_SF', 'city_DC', 'city_LA', 'city_NYC', 'city_Chicago', 'city_Boston','beds','accommodates','bedrooms','days_since_last_review','days_since_first_review'],axis=1)\n",
    "temp_test3.info()\n",
    "\n",
    "final_test = temp_test3.copy()\n",
    "# temp_test3 = final_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ WOrking on Test Dataset Complete =================================================\n",
    "\n",
    "# ================================ Model Building =============================================\n",
    "\n",
    "# Making number of columns in test and train equal.\n",
    "missing = []\n",
    "# Finding the column names missing in the Test set.\n",
    "for i in temp_train3.columns:\n",
    "    if i in temp_test3.columns:\n",
    "        continue\n",
    "    else:\n",
    "        missing.append(i)\n",
    "        \n",
    "# Creating new columns for the missing columns in test set and populating their values with zeroes.\n",
    "feature_difference_df = pd.DataFrame(data=np.zeros((temp_test3.shape[0], len(missing))),\n",
    "                                     columns=list(missing))\n",
    "# removing the log_price variable from test set.\n",
    "feature_difference_df=feature_difference_df.drop(['log_price'],axis=1)\n",
    "# Joining the missing variables to the test set.\n",
    "temp_test3 = temp_test3.join(feature_difference_df)\n",
    "#temp_test3 = temp_test3.drop(['property_type_Yurt'],axis=1)\n",
    "temp_test3.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================    CASE 1 All variables = Indepdendent variables   =====================================================\n",
    "\n",
    "\n",
    "# ========================    SPLITING THE TRAIN INTO TRAIN AND VALIDATE     =====================================================\n",
    "\n",
    "X = temp_train3.drop(['log_price'],axis=1)\n",
    "y = temp_train3['log_price']\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "def linearRegressionfunc(X_train,y_train,X_validate):\n",
    "    # create linear regression object \n",
    "    reg = linear_model.LinearRegression() \n",
    "    # train the model using the training sets \n",
    "    reg.fit(X_train, y_train)\n",
    "    reg_pred = reg.predict(X_validate) \n",
    "    print('The RMSE value for the Linear Regression model is ',\"%.4f\" % np.sqrt(metrics.mean_squared_error(y_validate,reg_pred)))  # Calculating RMSE for the model\n",
    "    r_sq = reg.score(X_train, y_train) # Calculating R2 for the model.\n",
    "    print('Coefficient of determination or R2 of Linear Regression Model is :', \"%.4f\" % reg.score(X_train, y_train))\n",
    "\n",
    "def OLSfunc(X_train,y_train):\n",
    "    est = sm.OLS(y_train, X_train.astype(float)).fit()\n",
    "    print(est.summary())\n",
    "    print(\"The R2 value of the OLS model is \", \"%.4f\" % est.rsquared)\n",
    "    return est\n",
    "\n",
    "# Finding the significant variables out of the total set. WILL USE FOR OTHER MODEL BUILDING LATER.\n",
    "def findingIndexOfSignificantVariables(est):\n",
    "    count = 0\n",
    "    index = -1\n",
    "    pos = []  # Getting index of variables in X which are not insignificant\n",
    "    for i in est.pvalues :\n",
    "        index = index + 1\n",
    "        if i < 0.05 : \n",
    "            count = count + 1\n",
    "            pos.append(index)\n",
    "    return pos\n",
    "\n",
    "# Getting the column names of the significant variables found in OLS.\n",
    "def gettingColNamesOfSignificantVariables(pos):\n",
    "    a=[] # Storing column names of significant variables in this variable.\n",
    "    for index, value in enumerate(X.columns):\n",
    "        if index in pos:\n",
    "            a.append(value)\n",
    "    return a\n",
    "\n",
    "def randomForestfunc(X_train,y_train,X_validate):\n",
    "    regr = RandomForestRegressor(max_depth=5, random_state=50)\n",
    "    regr.fit(X_train, y_train)\n",
    "    regr_pred = regr.predict(X_validate)\n",
    "    print('The RMSE value for the RandomForest model is ',\"%.4f\" % np.sqrt(metrics.mean_squared_error(y_validate,regr_pred)))  # Calculating RMSE for the model\n",
    "    print('Coefficient of determination or R2 value of RandomForest model is :', \"%.4f\" % regr.score(X_train, y_train))\n",
    "\n",
    "def XGBoostfunc(X_train,y_train,X_validate):\n",
    "    model = XGBRegressor(random_state=50)\n",
    "    model.fit(X_train, y_train)\n",
    "    model_pred = model.predict(X_validate)\n",
    "    print('The RMSE value for the XGBoost model is ',\"%.4f\" % np.sqrt(metrics.mean_squared_error(y_validate,model_pred)))  # Calculating RMSE for the model\n",
    "    print('Coefficient of determination or R2 value of XGBoost model is :', \"%.4f\" % model.score(X_train, y_train))\n",
    "        \n",
    "def lightGBMfunc(X_train,y_train,X_validate):\n",
    "    model = lgb.LGBMRegressor(random_state=50)\n",
    "    model.fit(X_train,y_train)\n",
    "    model_pred_light = model.predict(X_validate)\n",
    "    print('The RMSE value for the LightGBM model is ',\"%.4f\" % np.sqrt(metrics.mean_squared_error(y_validate,model_pred_light)))  # Calculating RMSE for the model\n",
    "    print('Coefficient of determination or R2 value of LightGBM model is :', \"%.4f\" % model.score(X_train, y_train))\n",
    "\n",
    "linearRegressionfunc(X_train,y_train,X_validate)\n",
    "est = OLSfunc(X_train,y_train)\n",
    "pos = findingIndexOfSignificantVariables(est)\n",
    "significant_colnames = gettingColNamesOfSignificantVariables(pos)\n",
    "randomForestfunc(X_train,y_train,X_validate)\n",
    "XGBoostfunc(X_train,y_train,X_validate)\n",
    "lightGBMfunc(X_train,y_train,X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================    CASE 2 Only Significant variables of OLS = Indepdendent variables   =====================================================\n",
    "\n",
    "new_train_X = X[significant_colnames]\n",
    "new_train_y =temp_train3[\"log_price\"]\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(new_train_X, new_train_y, test_size=0.20, random_state=42)\n",
    "\n",
    "linearRegressionfunc(X_train,y_train,X_validate)\n",
    "randomForestfunc(X_train, y_train, X_validate)\n",
    "XGBoostfunc(X_train, y_train, X_validate)\n",
    "lightGBMfunc(X_train, y_train, X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================    Final Model Chosen for Prediction => LightGBM with all variables as independent variables  =====================================================\n",
    "\n",
    "cols = temp_train3.drop(['log_price'],axis=1).columns\n",
    "temp_test3 = temp_test3[cols]\n",
    "\n",
    "X = temp_train3.drop(['log_price'],axis=1)\n",
    "y = temp_train3['log_price']\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# LightGBM\n",
    "model = lgb.LGBMRegressor(random_state=50)\n",
    "model.fit(X_train,y_train)\n",
    "model_pred_light = model.predict(temp_test3)\n",
    "\n",
    "temp_test3['log_price'] = model_pred_light\n",
    "df_test['log_price'] = model_pred_light\n",
    "\n",
    "df_test.to_csv(\"./Data/Final_Predicted_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
